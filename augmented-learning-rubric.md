# ATDD Harness 증강 학습(Augmented Learning) 평가 루브릭

> **기반 논문**: "How AI Impacts Skill Formation" (Judy Hanwen Shen, Alex Tamkin, Anthropic, 2026)
>
> **목적**: AI 도구 사용 시 생산성 향상과 스킬 형성을 동시에 달성하는 워크플로우 설계

---

## 1. 논문 핵심 발견 요약

### 1.1 AI 상호작용 패턴별 학습 결과

| 패턴 | 퀴즈 점수 | 평균 소요시간 | 특징 | 학습 보존 |
|------|-----------|---------------|------|-----------|
| **AI Delegation** | 39% | 19.5 min | AI에 완전 위임, 에러 거의 없음 | ❌ 손상 |
| **Progressive AI Reliance** | 35% | 22 min | 점진적 의존 증가, Task 2 개념 부족 | ❌ 손상 |
| **Iterative AI Debugging** | 24% | 31 min | AI로 디버깅만 반복, 질문 많음 | ❌ 손상 |
| **Conceptual Inquiry** | 65% | 22 min | 개념 질문만, 독자적 에러 해결 | ✅ 보존 |
| **Hybrid Code-Explanation** | 68% | 24 min | 코드 + 설명 동시 요청 | ✅ 보존 |
| **Generation-Then-Comprehension** | 86% | 24 min | 코드 생성 후 이해 질문 | ✅ 보존 |

### 1.2 핵심 인사이트

1. **디버깅 능력 저하가 가장 심각**
   - Control 그룹(No AI)이 AI 그룹보다 디버깅 문제에서 가장 큰 점수 차이
   - AI 감독 능력(Supervision Capability) 저하로 이어짐

2. **에러 경험이 학습에 필수**
   - Control 그룹: 평균 3개 에러 경험 → 개념 이해도 향상
   - AI 그룹: 평균 1개 에러 경험 → 개념 이해도 저하
   - Trio-specific 에러가 핵심 개념 습득을 강제

3. **인지적 참여가 학습 보존의 핵심**
   - 순수 위임: 생산성 ↑ 학습 ↓
   - 개념 질문 + 독자적 해결: 생산성 유지 + 학습 보존

4. **생산성 vs 학습 트레이드오프**
   - AI Delegation: 19.5분 완료 (가장 빠름) vs 39% 점수 (최저)
   - Generation-Then-Comprehension: 24분 완료 vs 86% 점수 (최고)

---

## 2. 평가 루브릭

### 2.1 평가 기준 (5점 척도)

| 점수 | 의미 |
|------|------|
| 5 | 완벽 구현 - 논문 권장사항 완전 반영 |
| 4 | 양호 - 대부분 반영, 일부 개선 여지 |
| 3 | 보통 - 부분적 반영, 명확한 개선 필요 |
| 2 | 부족 - 최소한의 반영, 상당한 개선 필요 |
| 1 | 미비 - 거의 반영되지 않음 |

### 2.2 세부 평가 항목

#### 카테고리 1: 인지적 참여 강제 (Cognitive Engagement) [가중치 30%]

| # | 항목 | 논문 근거 | 현재 상태 | 점수 | 개선 필요 |
|---|------|-----------|-----------|------|-----------|
| 1.1 | AI 위임 방지 메커니즘 | "AI Delegation은 학습 손상" | ADR Pre-Mortem, Self-Critique 존재 | 4 | Phase별 위임 방지 명시화 |
| 1.2 | 설명 요구 (Explanation First) | "코드 + 설명이 학습 보존" | Red Team Critique에 설명 포함 | 3 | 모든 코드 생성에 설명 강제화 |
| 1.3 | 질문 유형 가이드 | "개념 질문만 해도 65% 달성" | 부분적 (ADR에 존재) | 2 | 질문 유형 템플릿 추가 |
| 1.4 | 생성 후 이해 강제 | "Generation-Then-Comprehension 86%" | 없음 | 1 | 코드 생성 후 질문 강제 |

**카테고리 1 점수**: _____ / 5

---

#### 카테고리 2: 에러/디버깅 학습 기회 [가중치 25%]

| # | 항목 | 논문 근거 | 현재 상태 | 점수 | 개선 필요 |
|---|------|-----------|-----------|------|-----------|
| 2.1 | 독자적 에러 해결 기회 | "Control 그룹이 더 많은 에러 경험 = 더 높은 점수" | TDD에서 테스트 실패 경험 | 3 | 의도적 에러 유도 메커니즘 |
| 2.2 | 에러 분석 가이드 | "Trio-specific 에러가 개념 습득 강제" | 없음 | 1 | 에러 분석 워크플로우 추가 |
| 2.3 | AI 없이 디버깅 구간 | "AI Debugging 패턴은 24%로 최저" | Verify Phase에서만 | 2 | 중간 점검 구간 추가 |
| 2.4 | 에러 기반 학습 체크포인트 | "에러 경험이 핵심 개념 습득 강제" | 없음 | 1 | 에러 리포트 학습화 |

**카테고리 2 점수**: _____ / 5

---

#### 카테고리 3: 자기 성찰 & 메타인지 [가중치 20%]

| # | 항목 | 논문 근거 | 현재 상태 | 점수 | 개선 필요 |
|---|------|-----------|-----------|------|-----------|
| 3.1 | Self-Reflection 질문 | "왜 이렇게 설계했나요?" 질문 효과 | Red Team Design에 존재 | 4 | 전 Phase로 확장 |
| 3.2 | Self-Critique 루프 | "Self-Critique 후 품질 향상" | ADR Phase D에 존재 | 4 | 좋음 |
| 3.3 | 학습 메타인지 체크 | "사용자가 학습 부족을 인지하게 함" | 없음 | 1 | 학습 체크포인트 추가 |
| 3.4 | 피드백 기반 개선 | "즉각적 피드백이 학습 향상" | TDD 테스트 피드백 | 3 | 학습 피드백 자동화 |

**카테고리 3 점수**: _____ / 5

---

#### 카테고리 4: 생산성 vs 학습 균형 [가중치 15%]

| # | 항목 | 논문 근거 | 현재 상태 | 점수 | 개선 필요 |
|---|------|-----------|-----------|------|-----------|
| 4.1 | 생산성 측정 | 시간 단축 vs 학습 손상 트레이드오프 | 없음 | 1 | 생산성 메트릭 추가 |
| 4.2 | 학습 메트릭 | 퀴즈/평가로 학습 검증 | Verify Phase에서 간접적 | 2 | 직접적 학습 평가 추가 |
| 4.3 | 트레이드오프 시각화 | 사용자에게 생산성/학습 균형 안내 | 없음 | 1 | 대시보드/리포트 추가 |
| 4.4 | 모드 선택 (Speed vs Learning) | 사용자 의도에 따른 모드 전환 | 없음 | 1 | 학습 모드 설정 추가 |

**카테고리 4 점수**: _____ / 5

---

#### 카테고리 5: Compound Engineering 관점 [가중치 10%]

| # | 항목 | 논문 근거 | 현재 상태 | 점수 | 개선 필요 |
|---|------|-----------|-----------|------|-----------|
| 5.1 | 패턴 라이브러리화 | 학습 보존 패턴의 재사용 | 부분적 | 2 | 패턴 카탈로그화 |
| 5.2 | 난이도 조절 | "바람직한 어려움" 적용 | ADR에 적용됨 | 4 | 전 Phase 확장 |
| 5.3 | 피드백 루프 자동화 | 즉각적 피드백 → 학습 향상 | TDD 테스트 피드백 | 3 | 학습 피드백 자동화 |
| 5.4 | 워크플로우 적응성 | 프로젝트 특성에 맞는 조정 | 부분적 | 2 | 설정 기반 조정 |

**카테고리 5 점수**: _____ / 5

---

## 3. 현재 ATDD Harness 평가 결과

### 3.1 평가 점수

| 카테고리 | 점수 | 가중치 | 가중 점수 |
|----------|------|--------|-----------|
| 1. 인지적 참여 강제 | 2.5/5 | 30% | 0.75 |
| 2. 에러/디버깅 학습 | 1.75/5 | 25% | 0.44 |
| 3. 자기 성찰 & 메타인지 | 3.0/5 | 20% | 0.60 |
| 4. 생산성 vs 학습 균형 | 1.25/5 | 15% | 0.19 |
| 5. Compound Engineering | 2.75/5 | 10% | 0.28 |
| **총점** | | | **2.26/5.00** |

### 3.2 등급 판정

```
┌──────────────────────────────────────────────────────────────┐
│  증강 학습 성숙도 레벨                                        │
│                                                               │
│  Level 5: ⭐⭐⭐⭐⭐ 완전 자동화 (AI + 인지 참여 완벽 균형)      │
│  Level 4: ⭐⭐⭐⭐   최적화 (학습 메트릭 기반 지속 개선)         │
│  Level 3: ⭐⭐⭐     체계화 (학습 패턴 명시적 적용)   ← 목표    │
│  Level 2: ⭐⭐       부분적 (일부 Phase에만 적용)   ← 현재     │
│  Level 1: ⭐         초기 (인지 참여 메커니즘 부재)            │
└──────────────────────────────────────────────────────────────┘

현재 등급: C (보통)
목표 등급: B+ (체계화)
```

### 3.3 잘 설계된 부분

| 영역 | 구현 내용 | 논문 매핑 |
|------|-----------|-----------|
| **ADR Pre-Mortem** | "1년 후 실패 상상" → 깊은 사고 강제 | Self-Explanation |
| **ADR Self-Critique** | 5개 항목 자가 평가 (4점 이상 통과) | Retrieval Practice |
| **Red Team Critique** | 6관점 분석 + ACCEPT/DEFER/REJECT 결정 | Contrastive Cases |
| **Red Team Design Self-Reflection** | "왜 이 메서드가 이 Entity에?" 질문 | Meta-cognition |
| **TDD RED 단계** | 실패하는 테스트 먼저 작성 | Desirable Difficulties |

### 3.4 개선 필요 부분

| 영역 | 현재 상태 | 논문 시사점 | 개선 방안 |
|------|-----------|-------------|-----------|
| **질문 유형 가이드 없음** | 자유로운 AI 상호작용 | "개념 질문만 해도 65%" | 질문 유형 템플릿 추가 |
| **에러 학습 기회 부족** | AI가 에러 해결 가능 | "에러 경험 = 학습 향상" | 의도적 에러 발생 구간 |
| **AI 위임 감지 없음** | 위임 여부 모니터링 없음 | "AI Delegation = 39%" | 위임 패턴 감지 및 경고 |
| **학습 메트릭 없음** | 생산성만 측정 | "퀴즈로 학습 검증" | Phase별 학습 체크포인트 |
| **설명 강제화 없음** | 설명 없이 코드만 받기 가능 | "코드 + 설명 = 68%" | 모든 생성에 설명 요구 |

---

## 4. 개선 로드맵

### Phase 1: Quick Wins (1주차)

**목표**: 최소 변경으로 즉각적 효과

| # | 작업 | 상세 | 산출물 | 상태 |
|---|------|------|--------|------|
| 1.1 | 질문 유형 가이드 템플릿 | 개념형/생성형/디버깅형/설명형 분류 | `.atdd/templates/question-types.md` | ⬜ |
| 1.2 | AI 상호작용 로그 | 질문 유형, 횟수, 시간 기록 | `.atdd/logs/ai-interaction.log` | ⬜ |
| 1.3 | Phase별 학습 체크포인트 | "무엇을 배웠나요?" 질문 추가 | 각 Phase SKILL.md 수정 | ⬜ |
| 1.4 | 위임 패턴 경고 메시지 | 연속 코드 생성 요청 시 경고 | SKILL.md에 추가 | ⬜ |

### Phase 2: Core Enhancements (2주차)

**목표**: 핵심 학습 메커니즘 구현

| # | 작업 | 상세 | 산출물 | 상태 |
|---|------|------|--------|------|
| 2.1 | 에러 학습 구간 설계 | TDD Phase에 "AI 없이 5분 디버깅" 구간 | `WORKFLOWS.md` 수정 | ⬜ |
| 2.2 | 에러 분석 가이드 | 에러 발생 시 분석 템플릿 제공 | `.atdd/templates/error-analysis.md` | ⬜ |
| 2.3 | 설명 강제 메커니즘 | 코드 생성 시 설명 100자 이상 요구 | SKILL.md에 추가 | ⬜ |
| 2.4 | 위임 패턴 감지 로직 | 연속 3회 순수 위임 시 개념 질문 유도 | SKILL.md에 추가 | ⬜ |
| 2.5 | Generation-Then-Comprehension | 코드 생성 후 필수 질문 1개 이상 | SKILL.md에 추가 | ⬜ |

### Phase 3: Full Integration (3-4주차)

**목표**: 완전한 증강 학습 시스템

| # | 작업 | 상세 | 산출물 | 상태 |
|---|------|------|--------|------|
| 3.1 | 학습 메트릭 대시보드 | Phase별 학습 점수 시각화 | `.atdd/reports/learning-dashboard.md` | ⬜ |
| 3.2 | 생산성 vs 학습 트레이드오프 시각화 | 시간 단축 vs 이해도 그래프 | `.atdd/reports/tradeoff-report.md` | ⬜ |
| 3.3 | 패턴 카탈로그 라이브러리 | 6가지 패턴별 가이드 | `.atdd/patterns/` 디렉토리 | ⬜ |
| 3.4 | 학습 모드 설정 | standard/augmented/strict 모드 | `.atdd/config.yaml` | ⬜ |
| 3.5 | 학습 보존 패턴 자동 추천 | 위임 감지 시 대안 패턴 제안 | SKILL.md에 추가 | ⬜ |

---

## 5. 구체적 구현안

### 5.1 질문 유형 가이드 템플릿

```markdown
# AI 질문 유형 가이드

## 권장 질문 유형 (학습 보존)

### 1. 개념형 질문 (Conceptual Inquiry)
- "왜 이 패턴을 사용하나요?"
- "이 코드의 핵심 개념은 무엇인가요?"
- "어떤 상황에서 이 방식이 적합한가요?"
- **학습 효과**: 65% 퀴즈 점수 (단독 사용 시)

### 2. 설명형 질문 (Explanation Request)
- "이 코드가 어떻게 동작하는지 설명해주세요"
- "각 라인이 무엇을 하는지 설명해주세요"
- **학습 효과**: 68% 퀴즈 점수 (코드 + 설명)

### 3. 이해형 질문 (Comprehension Check)
- "이 코드를 사용하면 어떤 문제가 발생할 수 있나요?"
- "이 코드의 한계는 무엇인가요?"
- **학습 효과**: 86% 퀴즈 점수 (생성 후 이해)

## 주의 질문 유형 (학습 손상 위험)

### ⚠️ 순수 생성형 (Pure Generation)
- "이 코드를 짜주세요"
- "기능 구현해주세요"
- **위험**: AI Delegation 패턴 → 39% 퀴즈 점수
- **대안**: "이 기능을 구현해주세요. 각 단계를 설명해주세요"

### ⚠️ 순수 디버깅형 (Pure Debugging)
- "에러 고쳐주세요"
- "이 코드 왜 안되나요?"
- **위험**: AI Debugging 패턴 → 24% 퀴즈 점수
- **대안**: "이 에러의 원인이 무엇일까요?" (스스로 분석 후 확인)
```

### 5.2 학습 모드 설정

```yaml
# .atdd/config.yaml
learning_mode: augmented  # standard | augmented | strict

augmented_mode_settings:
  # 1. 질문 유형 가이드
  preferred_question_types:
    - conceptual      # "왜?" "어떻게?" 질문 권장
    - explanation     # "설명해줘" 질문 권장
  discouraged_question_types:
    - pure_generation # "코드만 짜줘" 질문 경고
    - pure_debugging  # "고쳐줘" 질문 경고

  # 2. 설명 강제
  require_explanation: true
  explanation_min_length: 100  # 최소 100자 설명

  # 3. 에러 학습 기회
  error_learning_checkpoints:
    - phase: tdd
      action: "AI 없이 5분간 에러 해결 시도"
    - phase: design
      action: "설계 결정 스스로 작성 후 AI 피드백"

  # 4. 학습 체크포인트
  learning_checkpoints:
    - phase: interview
      question: "이 요구사항의 핵심 도전 과제는?"
    - phase: design
      question: "왜 이 Entity 구조를 선택했나요?"
    - phase: tdd
      question: "이 테스트가 검증하는 것은?"

  # 5. 위임 패턴 감지
  delegation_detection:
    warning_threshold: 3  # 연속 3회 순수 위임 시 경고
    recovery_suggestion: "개념 질문을 먼저 해보세요"

standard_mode_settings:
  # 생산성 우선, 학습은 선택적
  require_explanation: false
  delegation_detection:
    warning_threshold: 10  # 관대한 설정

strict_mode_settings:
  # 학습 우선, 생산성 희생
  require_explanation: true
  explanation_min_length: 200
  mandatory_learning_checkpoints: true
  ai_free_zones:
    - phase: design
      duration: "10분"
      task: "설계 결정 스스로 작성"
```

### 5.3 에러 분석 템플릿

```markdown
# 에러 분석 템플릿

## 에러 정보
- **발생 시간**:
- **Phase**:
- **에러 메시지**:
- **에러 타입**: [ ] Syntax [ ] Logic [ ] Domain-specific

## 분석 (AI 질문 전에 작성)

### 1. 에러 원인 추측
내 생각: _________________________________

### 2. 관련 코드 라인
의심되는 코드: _________________________________

### 3. 시도한 해결 방법
1. _____________ → 결과: _____________
2. _____________ → 결과: _____________

### 4. AI에게 물어볼 질문 (개념형)
질문: _________________________________

## AI 응답 후

### 5. 내 추측 vs AI 설명 비교
- 맞은 부분:
- 틀린 부분:
- 새로 알게 된 것:

### 6. 학습 메모
다음에 비슷한 에러가 나면: _________________________________
```

### 5.4 학습 체크포인트 통합 예시

```markdown
# Phase X: [Phase명] Workflow

## 진입 조건
- ...

## 실행 흐름
1. ...
2. ...

## 🎯 학습 체크포인트 (Learning Checkpoint)

이 Phase를 완료하기 전에 스스로 질문해보세요:

| # | 질문 | 내 답변 |
|---|------|---------|
| 1 | 이 Phase에서 가장 중요한 개념은? | |
| 2 | 가장 어려웠던 부분은? | |
| 3 | AI에게 어떤 질문을 했나요? | |
| 4 | AI 없이 스스로 해결한 것은? | |

### 학습 점수 자가 평가
- [ ] 5점: AI 없이 완전히 이해함
- [ ] 4점: AI 개념 질문만으로 이해함
- [ ] 3점: AI 코드 + 설명으로 이해함
- [ ] 2점: AI 코드 생성 후 추가 질문 필요
- [ ] 1점: AI에 완전히 의존함

⚠️ 2점 이하라면 개념을 다시 복습하세요.
```

---

## 6. 메트릭 및 측정

### 6.1 수집할 메트릭

| 메트릭 | 측정 방법 | 목표 |
|--------|-----------|------|
| **질문 유형 분포** | 개념형/생성형/디버깅형 비율 | 개념형 > 50% |
| **에러 독립 해결률** | AI 도움 없이 해결한 에러 비율 | > 60% |
| **학습 체크포인트 점수** | Phase별 자가 평가 평균 | > 3.5 |
| **위임 패턴 발생 횟수** | 연속 순수 위임 횟수 | < 3회 |
| **설명 요구 충족률** | 100자 이상 설명 포함 비율 | 100% |

### 6.2 학습 메트릭 리포트 템플릿

```markdown
# 학습 메트릭 리포트

## 프로젝트: [프로젝트명]
## 기간: [시작일] ~ [종료일]

### 요약

| 지표 | 값 | 목표 | 달성 여부 |
|------|-----|------|-----------|
| 평균 학습 점수 | 3.8 | 3.5 | ✅ |
| 개념형 질문 비율 | 45% | 50% | ⚠️ |
| 에러 독립 해결률 | 55% | 60% | ⚠️ |
| 위임 패턴 발생 | 2회 | < 3회 | ✅ |

### Phase별 상세

#### Phase 1: Interview
- 학습 점수: 4.2
- 주요 학습: 요구사항 분석 능력
- 개선 필요: -

#### Phase 2: Design
- 학습 점수: 3.5
- 주요 학습: Entity 관계 설계
- 개선 필요: AI 위임 감소 필요

### 권장 사항
1. Design Phase에서 개념형 질문 늘리기
2. 에러 발생 시 5분간 독립 해결 시도 늘리기
```

---

## 7. 참고 자료

### 7.1 논문 참조

- **원문**: "How AI Impacts Skill Formation" (Judy Hanwen Shen, Alex Tamkin, Anthropic, 2026)
- **arXiv**: 2601.20245v2
- **요약문**: `how_ai_impacts_skill_formation.md`

### 7.2 관련 파일

- `WORKFLOWS.md`: 현재 워크플로우 정의
- `AGENTS.md`: 각 Phase별 Agent 정의
- `TEMPLATES.md`: 산출물 템플릿

### 7.3 추가 학습 자료

- Robert Bjork - "Desirable Difficulties"
- Kolb - "Experiential Learning Cycle"
- Problem-Based Learning (PBL)

---

## 8. 변경 이력

| 날짜 | 버전 | 변경 내용 | 작성자 |
|------|------|-----------|--------|
| 2026-02-19 | 1.0 | 초기 버전 | - |
| | | | |

---

*이 루브릭은 논문 "How AI Impacts Skill Formation"을 기반으로 ATDD Harness의 증강 학습 지원 정도를 평가하기 위해 작성되었습니다.*
